<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[python爬虫之爬取静态网页（实战篇）]]></title>
    <url>%2F2018%2F02%2F24%2Fpython%E7%88%AC%E8%99%AB%E4%B9%8B%E7%88%AC%E5%8F%96%E9%9D%99%E6%80%81%E7%BD%91%E9%A1%B5%E5%AE%9E%E6%88%98%E7%AF%87%2F</url>
    <content type="text"><![CDATA[以爬取桌酷网下桌面最新壁纸一栏中的图片为例。本篇教程旨在让新手快速入门爬虫，所以不涉及面向对象编程。12345678910111213141516# encoding:utf-8import requestsfrom bs4 import BeautifulSoupimport osfrom random import choicefrom time import sleepurl = &quot;http://www.zhuoku.com/&quot;header_list = [ &quot;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.91 Safari/537.36&quot;, &quot;Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50&quot;, &quot;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)&quot;, &quot;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; 360SE)&quot;]header1 = &#123;&quot;User-Agent&quot;:&quot;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)&quot;&#125;header1 = &#123;&quot;User-Agent&quot;:&quot;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)&quot;&#125; 先导入必要的库，配置该项爬虫的基本设置。在header_list中存放几个User-Agent反爬。123456try: r = requests.get(url,headers=header1,timeout=30)except: r = requests.get(url,headers=header1,timeout=30)r.encoding = &apos;utf-8&apos; 利用requests.get获取桌酷网首页的html文件，并将其设置为utf-8格式。timeout设置为30秒，如果30秒内没有获取成功会报错，为了避免连接超时导致程序停止在except中让程序再次连接这个url12soup = BeautifulSoup(res,&quot;html.parser&quot;)all_a = soup.find(&quot;div&quot;,id=&quot;zuixin&quot;).find_all(&apos;a&apos;,attrs=&#123;&quot;class&quot;:&quot;title&quot;&#125;) 煮一锅“汤”，解析下载好的html文件。在文件中找到‘zuixin’下所有分类为‘title’的a标签。12345for a in all_a: header = &#123;&quot;User-Agent&quot;: choice(header_list)&#125; title = a.get_text().replace(&quot;/&quot;,&apos;&apos;) href = a.get(&quot;href&quot;) img_url = url + href[1:-4] + &quot;(1).htm&quot;#补第一张图片的全href 值得注意的是，此时我们用choice()函数从上面设置好的header_list列表中随机选择了一个User-Agent作为header里的内容。在下面的每一次requests.get()中都会加上这个header，这样就可以突破很多网站的反爬措施了。遍历找出的每一个a标签,通过get_text()方法取出该标签的元素作为这组图片的title，通过get(‘href’)取出该a标签的href属性内容。通过观察可以发现，这组图片的url就是网站首页url与取出的href属性拼接而成，后面(1).htm这个后缀实际上代表了这组图片的页数。(1).htm既是这组图的第一张图片，(2).htm既是这组图的第二张图片…通过补全url构造出每一组图的第一张图片url。12345678910111213if os.path.isdir(os.path.join(&quot;D:\zhuoku&quot;,title)): #如果存在文件夹 print(&quot;exist&quot; + title) passelse: os.makedirs(os.path.join(&quot;D:\zhuoku&quot;,title)) #创建文件夹 print(&quot;makedir&quot; + title)os.chdir(&quot;D:\zhuoku\\&quot; + title) #切换到此文件夹try: img_url_get = requests.get(img_url,headers=header,timeout=30)except: img_url_get = requests.get(img_url,headers=header,timeout=30)sleep(0.5) 判断一下储存图片的文件夹是否存在，不存在就创建一个（os.join()将两个路径拼接起来），然后通过os.chdir()移动到这个文件夹下。还是通过requests.get()获取这张图片页面的html文件。sleep()让程序停止0.5秒，免得访问太频繁给服务器带来太大的负担。设置合适的sleep()时间很重要，既是出于不给网站带来太大压力的道德因素，也是避免访问过于频繁导致服务器强制断开连接使程序中断1234567891011img_url_soup = BeautifulSoup(img_url_get.text,&quot;html.parser&quot;)# 找出最大页数max_img_page = img_url_soup.find(&apos;div&apos;,id=&quot;yema&quot;).find_all(&quot;a&quot;)[-1].get_text()for page in range(1,int(max_img_page)+1): jpg_href = url + href[1:-4] + &quot;(&quot; + str(page) + &quot;).htm&quot; + &quot;#turn&quot; try: jpg_href_get = requests.get(jpg_href,headers=header,timeout=30) except: jpg_href_get = requests.get(jpg_href,headers=header,timeout=30) sleep(0.5) 解析下载好的html文件，在html文件中找到这组图片的最大页数。依次构造这组图的每一张图片的url。通过1234jpg_soup = BeautifulSoup(jpg_href_get.text,&quot;html.parser&quot;)# 在find()方法后面用方括号将属性括起来可以取出该属性的值jpg_url = jpg_soup.find(&quot;div&quot;,id=&quot;bizhiimg&quot;).find(&quot;img&quot;)[&quot;src&quot;] name = jpg_url[-9:] #截取倒数第九位至末尾为图片的名字 依次解析每一张图片的html文件，通过find(‘img’)找出图片所在的img标签并取出src属性的值（类似于get(‘src’)），这个值就是图片URI。12345678910111213141516171819 if os.path.isfile(name): #如果存在名为name的文件 print(name + &quot; exist skip&quot;) pass #下面全跳过 else: jpg_header = &#123; &quot;Referer&quot;: jpg_href, &quot;User-Agent&quot;:choice(header_list) &#125; try: jpg = requests.get(jpg_url,headers=jpg_header,timeout=30) except: jpg = requests.get(jpg_url,headers=header,timeout=30) sleep(0.5) with open(name,&apos;wb&apos;) as f: f.write(jpg.content) print(name+&quot; saved&quot;)print(&quot;congratulations! all finished!&quot;) 判断一下图片是否已经保存。如果没有保存通过requests.get()访问刚才找出的URI，此时变量jpg就可以看做是这张图片。以‘wb’形式新建一个名为name变量的值的文件，通过f.write(jpg.content)将图片写入文件中。成功！现在你就有一个完整的爬虫了！启动程序，大概20分钟之后，这些图片就会统统下载到你的电脑里啦~以下是程序的完整代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273# encoding:utf-8import requestsfrom bs4 import BeautifulSoupimport osfrom random import choicefrom time import sleepurl = &quot;http://www.zhuoku.com/&quot;header_list = [ &quot;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.91 Safari/537.36&quot;, &quot;Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50&quot;, &quot;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)&quot;, &quot;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; 360SE)&quot;]header1 = &#123;&quot;User-Agent&quot;:&quot;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)&quot;&#125;try: r = requests.get(url,headers=header1,timeout=30)except: r = requests.get(url,headers=header1,timeout=30)r.encoding = &apos;utf-8&apos;soup = BeautifulSoup(r,&quot;html.parser&quot;)all_a = soup.find(&quot;div&quot;,id=&quot;zuixin&quot;).find_all(&apos;a&apos;,attrs=&#123;&quot;class&quot;:&quot;title&quot;&#125;)for a in all_a: header = &#123;&quot;User-Agent&quot;: choice(header_list)&#125; title = a.get_text().replace(&quot;/&quot;,&apos;&apos;) href = a.get(&quot;href&quot;) img_url = url + href[1:-4] + &quot;(1).htm&quot;#补第一张图片的全href if os.path.isdir(os.path.join(&quot;D:\zhuoku&quot;,title)): #如果存在文件夹 print(&quot;exist&quot; + title) pass else: os.makedirs(os.path.join(&quot;D:\zhuoku&quot;,title)) #创建文件夹 print(&quot;makedir&quot; + title) os.chdir(&quot;D:\zhuoku\\&quot; + title) #切换到此文件夹 try: img_url_get = requests.get(img_url,headers=header,timeout=30) except: img_url_get = requests.get(img_url,headers=header,timeout=30) sleep(0.5) img_url_soup = BeautifulSoup(img_url_get.text,&quot;html.parser&quot;) max_img_page = img_url_soup.find(&apos;div&apos;,id=&quot;yema&quot;).find_all(&quot;a&quot;)[-1].get_text() for page in range(1,int(max_img_page)+1): jpg_href = url + href[1:-4] + &quot;(&quot; + str(page) + &quot;).htm&quot; + &quot;#turn&quot; try: jpg_href_get = requests.get(jpg_href,headers=header,timeout=30) except: jpg_href_get = requests.get(jpg_href,headers=header,timeout=30) sleep(0.5) jpg_soup = BeautifulSoup(jpg_href_get.text,&quot;html.parser&quot;) jpg_url = jpg_soup.find(&quot;div&quot;,id=&quot;bizhiimg&quot;).find(&quot;img&quot;)[&quot;src&quot;] #在find方法后面用方括号将属性括起来可以取出该属性的值 name = jpg_url[-9:] #截取倒数第九位至末尾为图片的名字 if os.path.isfile(name): #如果存在名为name的文件 print(name + &quot; exist skip&quot;) pass #下面全跳过 else: jpg_header = &#123; &quot;Referer&quot;: jpg_href, &quot;User-Agent&quot;:choice(header_list) &#125; try: jpg = requests.get(jpg_url,headers=jpg_header,timeout=30) except: jpg = requests.get(jpg_url,headers=header,timeout=30) sleep(0.5) with open(name,&apos;wb&apos;) as f: f.write(jpg.content) print(name+&quot; saved&quot;)print(&quot;congratulations! all finished!&quot;)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqlalchemy框架下的数据库关系]]></title>
    <url>%2F2018%2F02%2F13%2Fsqlalchemy%E6%A1%86%E6%9E%B6%E4%B8%8B%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93%E5%85%B3%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[关系型数据库中表的关系一对多考虑下面的代码12345678910111213141516171819202122232425from flask-sqlalchemy import SQLAlchemyfrom flask import Flaskimport osapp = Flask(__name__)# 配置app，实例化SQLAlchemybasedir = os.path.abspath(os.path.dirname(__file__))app.config[&apos;SQLALCHEMY_DATABASE_URI&apos;] = &apos;sqlite:///&apos; + os.path.join(basedir + &apos;db.sqlite&apos;)app.config[&apos;SECRECT_KEY&apos;] = &apos;a secrect string&apos;db = SQLAlchemy(app)# 定义模型class Writer(db.model): __tablename__ = writers id = db.Column(db.Integer,primary_key=True) name = db.Column(db.String(64),unique=True) posts = db.relationship(&apos;articles&apos;,backref=&apos;writer&apos;)class Article(db.model): __tablename__ = articles id = db.Column(db.Integer,primary_key) title = db.Column(db.String(64)) body = db.Column(db.String(2018)) writer_id = db.Colum(db.Integer,ForeignKey=&apos;writers.id&apos;) 在Writer模型中，relationship定义了两张表之间的关系.relationship的backref属性相当于在另一张表中也定义了一个相关关系。这样Writer实例通过posts属性可以访问所有与之相关的Article模型，返回一个关联Article组成的列表。Article实例通过writer属性可以访问对应的Writer模型。 在Article模型中，writer_id属性被定义为外键，其值通过ForeignKey指向Writer模型中的主键。实例化模型123susan = writer(name=susan)love_python = article(title=&apos;love python&apos;,body=&apos;python is easy and \elegant&apos;,writer=susan) 在实例化Article模型时，由relationship反向定义的writer属性要传入与love_python对应的的Writer实例susan。这样就可以让love_python与susan两个实例关联起来。 一对一要让两张表是一对一关系，只需要在relationship中将uselist设为False。 多对一与一对多类似，只需要对调两张表。]]></content>
      <categories>
        <category>python 数据库</category>
      </categories>
      <tags>
        <tag>杂谈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python杂谈]]></title>
    <url>%2F2018%2F02%2F07%2Fpython%E6%9D%82%E8%B0%88%2F</url>
    <content type="text"><![CDATA[面向对象编程之方法与函数面向对象编程思想中，方法是指一个对象可以使用的功能，举个例子12arr = [1,2,3]arr.remove(2) arr被定义为指向列表对象的变量，而remove则是列表对象的一个方法，用于从列表中删除某个值。而函数则是对对象进行操作，例如12arr = [1,2,3]len(arr) len()函数对其参数求长度。在本例中len()的参数既是一个列表。 python常用的装饰器python中的装饰器可以对函数进行扩展。 @property@property装饰器可以把类的方法变成属性。例如1234567class Student(): def __init__(self,score): self.score = score#调用Student类qiuyue = Student(90)print(qiuyue.score) #输出结果90 可是这样也不无问题，比如当输入分数不合理（如1000）时，无法对分数进行检查。当然，可以对Student类附加方法实现检查分数。123456789101112131415class Student(): def get_score(self): return self._score def set_score(self,value): if not isinstance(score,int): raise ValueError(&apos;score must is an integer!&apos;) if value &gt; 100 or value &lt; 0: raise ValueError(&apos;score must between 0 to 100!&apos;) self._score = value#调用Student类qiuyue = Student()qiuyue.set_score(90)print(qiuyue.get_score()) #输出90 通过set方法对Student的score属性赋值，再用get方法获取score属性。这样就可以实现对score合法性的检查。但是为了一个属性特地写两个方法未免过于繁琐。所以用到装饰器@property封装set方法和get方法，实现对score属性赋值的同时进行数值合法性检查。 12345678910111213141516171819202122232425262728293031class Student(): @property def score(self) return self._score @score.setter def score(self,value) if not isinstance(value,int): raise ValueError(&apos;score must is an integer!&apos;) if value &gt; 100 or value &lt; 0: raise ValueError(&apos;score must between 0 to 100!&apos;) self._score = value @property def grade(self): if self._score is None: return None elif self._score &gt;= 90: print(&apos;优秀！&apos;) elif self._score &gt;=60: print(&apos;及格！) else: print(&apos;不及格！&apos;)#调用Student类qiuyue = Student()qiuyue.score = 90print(qiuyue.score) # 输出90qiuyue.score = 1000 # 报错，ValueErrorqiuyue.grade = &apos;及格&apos; # 报错qiuyue.grade # 输出优秀 可以看到，@property装饰的第一个score，实际上是一个get方法，而@score.setter装饰的第二个score实际上是set方法。@score.setter其实是@property装饰器的副产品。这两个装饰器一个装饰get方法，一个装饰set方法，这样就使score方法变成了Student类的属性，在对score属性赋值（即set方法）时会自动对值的合法性进行检查，调用score属性即调用get方法。@grade.setter并不是必须的，当缺少@grade.setter装饰器时grade属性变成只读属性，无法对其进行赋值，只能读取。 @classmethod与@staticmethod在介绍类方法@classmethod与静态方法staticmethod前，先要清楚一个概念：类属性与实例属性是不同的。12345678class Plus(): num = 1f = Plus()f.num = 2print(Plus.num) # 结果为2 print(f.num) # 结果为1 众所周知，要调用类的方法，我们首先要把这个类实例化，然后通过实例名.方法名的方式调用。而@classmethod与@staticmethod都可以实现通过类名.方法名的方式调用方法。 @classmethod当类中有些方法不需要涉及实例，而需要涉及类，如对类属性的修改，往往使用@classmethod。用@classmethod修饰的方法不会将实例传入方法中，而会自动将自身类作为第一个参数传入。所以这个方法不需要写self参数，但需要一个cls参数代表这个类。12345678class Apple(): apple = 1 @classmethod def how_much(cls): if cls.app == 1: print(&apos;还有一个苹果&apos;)Apple.how_much() # 输出：还有一个苹果 @staticmethod如果类中有些方法既不涉及类，也不涉及实例，可以用@staticmethod。@staticmethod既不会将实例传入方法，也不会将自身类传入方法。所以既没有self参数也没有cls参数。12345678910111213class Apple(): apple = 1 def change(self,data): self.apple = data print(&apos;还有%s个苹果&apos; % self.apple) @staticmethod def how_much(): print(&apos;没有苹果了&apos;)apple = Apple()apple.change(2) # 输出：还有2个苹果Apple.how_much() # 输出： 没有苹果了 下面这个例子加深区分：1234567891011121314151617181920class Apple(): num = 1 def __init__(self,data): self.num = data def common(self): print(&apos;还有%s个苹果&apos; % self.num) @classmethod def clsmed(cls): print(&apos;还有%s个苹果&apos; % cls.num) @staticmethod def stamed(): print(&apos;没有苹果了&apos;)apple = Apple(2)apple.common() # 输出：还有2个苹果Apple.clsmed() # 输出：还有1个苹果Apple.stamed() # 输出：没有苹果了]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>杂谈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫之爬取静态网页(知识储备篇)]]></title>
    <url>%2F2018%2F01%2F29%2Fpython%E7%88%AC%E8%99%AB%E4%B9%8B%E7%88%AC%E5%8F%96%E9%9D%99%E6%80%81%E7%BD%91%E9%A1%B5%2F</url>
    <content type="text"><![CDATA[断断续续地学习爬虫也有一段时间了。最近接到点石的通知在寒假会开始Web后端方向的学习，趁着这几天有空抓紧把已经学会的知识整理一下。 HTTP相关知识客户端与服务器之间通过TCP/IP等协议进行HTTP报文的传输。HTTP报文又分为请求报文（客户端发送至服务器）和响应报文（服务器发送至客户端）。无论是哪种报文，其格式都是大同小异的。下面进行通过《图解HTTP》上的一张图片进行简单介绍上图分别是一个请求报文和一个响应报文。请求报文又分成两部分：请求头（GET/HTTP/1.1）和请求首部（XXXX:XXXX）。get是一种请求方法，表示请求从服务器获得信息，除此之外还有post方法（从服务器获得信息的同时向服务器传递一些信息），put方法（向服务器传递信息），delect方法（从服务器删除信息）等。HTTP/1.1表示该请求使用的是1.1版本的HTTP协议。而首部字段则包含了本次请求的有关信息，如Host表示本次请求的主机地址，User-Agent表示发出请求的浏览器内核信息，Accept-luangage表示浏览器接受的语言等。响应报文的响应头是HTTP/1.1 200 OK。200是一种状态码，表示成功，4xx表示客户端错误，5xx表示服务器错误，3xx表示重定向。响应首部字段内容与请求首部作业相同，报文主体里则包含了本次响应的具体信息，通常是HTML文档。更多有关http知识可以看《图解http》这本书，有大量的图例，内容较为浅显，适合新手入门。 requests库相关知识requests库是python爬虫常用库,常用于下载网页的html代码。使用上相比urllib要方便快捷得多。可以从官方文档获取有关这个库的详细使用方法requests文档 BeautifulSoup库相关知识BeautifulSoup库常用于解析下载好的HTML，通常与requests库配合使用。可以通过官方文档学习有关知识BeautifulSoup文档值得一提的是其自带了html_parser解析器，也可以在cmd下通过 pip isntall lxml 安装lxml解析器，由于lxml是用C写的，所以解析速度比html_parser快，但是用lxml作为解析器有时候会出现解析出来的HTML文档部分丢失的情况。 HTML相关知识HTML是网页的骨架，js，css都要有在HTML的基础才能进行开发。即使是Web后端也要对HTML有简单的认识。对于这部分的知识可以去W3school进行学习，里面还有关于JavaScript的教程，也是知识体系中很重要的一环，可以一并学习。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进程与线程]]></title>
    <url>%2F2018%2F01%2F09%2F%E8%BF%9B%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[开进程需要时间学习《python爬虫开发与项目实践》时，执行下面一段代码：1234567891011121314from multiprocessing import Processimport osdef run_process(name): print(&quot;Child process %s (%s) is running&quot; % (name,os.getpid()))if __name__ == &quot;__main__&quot;: print(&quot;parant process %s &quot; % os.getpid()) for i in range(5): p = Process(target=run_process, args=(str(i),)) print(&quot;process will start&quot;) p.start() p.join() print(&quot;process end&quot;) 显示的结果是123456789101112parant process 6332 process will startprocess will startprocess will startprocess will startprocess will startChild process 2 (9896) is runningChild process 0 (11208) is runningChild process 3 (5464) is runningChild process 1 (10208) is runningChild process 4 (12596) is runningprocess end 可以看到，程序在执行完1print (&quot;parant process %s &quot; % os.getpid()) 没有接着马上执行run_process()，而是先打印process will start，最后把子进程一起执行。这是因为子进程的创建是需要时间的，在这个空闲时间里父进程继续执行代码，而子进程在创建完成后显示。 Pool进程池需要创建多个进程时，可以使用multiprocessing中的Pool类开进程池。Pool()默认开启数量等于当前cpu核心数的子进程（当然可以手动改变）1234567891011121314from multiprocessing import Pooldef hello(i): print(&quot;hello ,this is the %d process&quot; % i)def main(): p = Pool() for i in range(1,5): p.apply_async(targe=hell0,args=(i,)) p.close p.joinif __name__ == &quot;__main__&quot;: main() apply_async表示在开进程时不阻塞主进程，是异步IO的一种方式之一。targe传入要在子线程中执行的函数（约定此时函数不用带括号），args以元组的方式传入函数的参数。join会等待线程池中的每一个线程执行完毕，在调用join之前必须要先调用close，close表示不能再向线程池中添加新的process了。 线程我们是没有办法完全认为控制线程的，因为线程由系统控制。但是可以用一些方式来影响线程的调用，比如互斥锁，sleep（阻塞），死锁等。 线程的几种状态新建—–就绪—————-运行—–死亡&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;等待（阻塞）线程的生命周期由run方法决定，当run()方法结束时线程死亡。123456789from threading import Threadclass MyThread(Thread): def run(self): print(&apos;i am sorry&apos;)if __name__ == &apos;__main__&apos;: t = MyThread() t.start() 可以通过继承Thread，重写run方法改变Thread的功能，最后还是通过start()方法开线程。通过args参数以一个元组的方式给线程中的函数传参。12345678from threading import Threaddef sorry(name): print(&apos;i am sorry&apos;,name)if __name__ == &apos;__main__&apos;: t = Thread(target=sorry,args=(mike)) t.start() 线程锁多线程中任务中，可能会发生多个线程同时对一个公共资源（如全局变量）进行操作的情况，这是就会发生混乱。为了避免这种情况，需要引入线程锁的概念。只有一个线程能处于上锁状态，当一个线程上锁之后，如果有另外一个线程试图获得锁，该线程就会挂起直到拥有锁的线程将锁释放。这样就保证了同时只有一个线程对公共资源进行访问或修改。12345678910111213141516171819from threading import Thread,Locknum = 0def puls(): # 获得一个锁 lock = Lock() global num # acquire()方法上锁 lock.acquire() num += 1 print(num) # release()方法解锁 lock.release()if __name__ == &apos;__main__&apos;: for i in range(5): t = Thread(target=plus) t.start() t.join() join()方法会阻塞主线程直到子线程全部结束（也就是同步）。锁的用处: 确保某段关键代码只能由一个线程从头到尾执行，保证了数据的唯一性。 锁的坏处: 阻止了多线程并发执行，效率大大降低。 由于存在多个锁，不同的线程持有不同的锁并试图获取对方的锁时，可能造成死锁。守护线程线程其实并没有主次的概念，我们一般说的‘主线程’实际上是main函数的线程，而所谓主线程结束子线程也会结束是因为在主线程结束时调用了系统的退出函数。而守护线程是指‘不重要线程’。主线程会等所有‘重要’线程结束后才结束。通常当客户端访问服务器时会为这次访问开启一个守护线程。将setDaemon属性设为True即可将该线程设为守护线程。123456789101112from threading import Threadn = 100def count(x,y): n=x+yif __name__ == &apos;__main__&apos;: t = Thread(target=count,args=(1,2)) t.setDaemon = True # ...]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>爬虫 web开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[http学习]]></title>
    <url>%2F2017%2F12%2F17%2Fhttp%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[TCP/IP与DNS什么是TCP/IP?协议中存在各式各样的内容。从电缆的规格到 IP 地址的选定方法、寻找异地用户的方法、双方建立通信的顺序，以及 Web 页面显示需要处理的步骤，等等。像这样把与互联网相关联的协议集合起来总称为 TCP/IP。HTTP 属于它内部的一个子集。也有说法认为，TCP/IP 是指 TCP 和 IP 这两种协议。还有一种说法认为，TCP/IP 是在 IP 协议的通信过程中，使用到的协议族的统称。 IPIP与IP地址不同。IP是一种网络协议，它的作用是把数据包传给对方。具体是通过MAC地址和ip地址来实现的。MAC地址对应网卡所属的固定地址，ip地址指节点被分配到的地址。IP地址可以与MAC地址对应，IP地址可以换，MAC地址一般是固定的。IP 间的通信依赖 MAC 地址。在网络上，通信的双方在同一局域网（LAN）内的情况是很少的，通常是经过多台计算机和网络设备中转才能连接到对方。而在进行中转时，会利用下一站中转设备的 MAC 地址来搜索下一个中转目标。这时，会采用 ARP 协议（AddressResolution Protocol）。ARP 是一种用以解析地址的协议，根据通信方的 IP 地址就可以反查出对应的 MAC 地址。 TCP所谓的字节流服务（Byte Stream Service）是指，为了方便传输，将大块数据分割成以报文段（segment）为单位的数据包进行管理。而可靠的传输服务是指，能够把数据准确可靠地传给对方。一言以蔽之，TCP 协议为了更容易传送大数据才把数据分割，而且 TCP 协议能够确认数据最终是否送达到对方。 TCP的三次握手TCP把数据包发出去之后还会确认数据到达了目的地，通过三次握手机制进行确认。发送端发送数据时，TCP会向服务器发送带有SYN标记的数据包，当服务器接收到这个数据包后，会返回一个带有SYN或者ASK的数据包表示确认，最后发送端会再次发送带有ASK标志的数据包表示握手结束。 DNSDNS（Domain Name System）服务是和 HTTP 协议一样位于应用层的协议。它提供域名到 IP 地址之间的解析服务。计算机既可以被赋予 IP 地址，也可以被赋予主机名和域名。比如 www.baidu.com。用户通常使用主机名或域名来访问对方的计算机，而不是直接通过 IP 地址访问。因为与 IP 地址的一组纯数字相比，用字母配合数字的表示形式来指定计算机名更符合人类的记忆习惯。但要让计算机去理解名称，相对而言就变得困难了。因为计算机更擅长处理一长串数字。为了解决上述的问题，DNS 服务应运而生。DNS 协议提供通过域名查找 IP 地址，或逆向从 IP 地址反查域名的服务。 综上，从输入网址到服务器获得请求的过程是：]]></content>
      <categories>
        <category>http</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重新开始]]></title>
    <url>%2F2017%2F12%2F02%2F%E9%87%8D%E6%96%B0%E5%BC%80%E5%A7%8B%2F</url>
    <content type="text"><![CDATA[前阵子手贱删了博客文件，还糊里糊涂地把coding里的项目也删了。然后各种蜜汁错误，各种无法重新部署。最近几天又蜜汁部署成功。可以说十分难受了。 不过塞翁失马，焉知非福。经过这么一遭我再次练习了一遍coding+hexo下博客的部署，也算是好事一桩了吧？（强行自我安慰一波233）以后我一定天天向上，重新做人，再不手贱。 再次感谢王哥的教程，很详细，帮助很大，很好，很棒。感兴趣的同志可以去他那里转转呀（手动滑稽）windliang的博客扯到这里算是暂时结束，以后有时间再上来扯扯淡，写点学习心得啥的吧。]]></content>
      <categories>
        <category>心得</category>
      </categories>
      <tags>
        <tag>闲聊</tag>
      </tags>
  </entry>
</search>
