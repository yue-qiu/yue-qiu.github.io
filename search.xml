<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[flask学习]]></title>
    <url>%2F2018%2F03%2F22%2Fflask%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[flask的4种请求钩子有时在处理请求之前或者之后执行代码会很有用。例如：在请求开始时，我们可能需要创建数据库连接或者认证发起请求的用户，为了避免在每个视图函数中都有重复的代码，Flask提供了注册通用函数的功能，注册的函数可以在请求被分别发布到视图函数之前或之后自动调用。这种注册函数的功能就称之为“请求钩子”。 @before_first_request注册一个函数，在处理第一个请求之前执行。 @before_request注册一个函数，在每次请求之前执行。1234567891011from flask import Flaskapp = Flask(__name__)@app.route('/')def index(): return 'hi'@app.before_requestdef hello(): print('hello') @after_request注册一个函数，如果没有未处理的异常抛出，在每次请求之后执行。 @teardown_request注册一个函数，即使有未处理的异常抛出，也在每次请求之后运行。 flask的4种上下文全局变量请求上下文(request context)request请求对象，封装了客户端发出的HTTP请求中的内容。12345678910from flask import request,Flask,abortapp = Flask(__name__)@app.route('/',methods=['GET','POST'])def index(): if request.method == 'GET': return 'hello' else: abort(404) 这段代码表示当客户端以GET的方式发送HTTP请求时，会返回hello，否则abort()会抛出一个404错误。 request获取HTTP请求内容的几种方式 request.method获取HTTP请求方式 request.form.get(‘name’)获取form中的内容，当name不存在时返回None。(注意：用request.form[‘name’]也可以把内容取出来，但是任何时候都不应该用这种危险的方式，因为当name不存在时会直接抛出错误，下文同理) request.args.get(‘name’)获取以查询字符串中的内容，当name不存在时返回None。 request.cookies.get(‘name’)获取cookies中的内容，当name不存在时返回None。 request.headers.get(‘name’)获取HTTP请求的header中的内容，当name不存在时返回None。 request.url/base_url/path/url_root获取请求url相关内容，直接上结果方便理解：123return &apos;url: %s , path: %s , base_url: %s , url_root : %s&apos; % (request.url,request.script_root, request.path,request.base_url,request.url_root)#url: http://192.168.1.183:5000/testrequest?a&amp;b ,path: /testrequest , base_url: http://192.168.1.183:5000/testrequest , url_root : http://192.168.1.183:5000/ gg：global。处理请求时用作临时存储的对象，每次请求都会重设这个变量。g还常用于在请求钩子和视图函数之间共享数据。例如：before_request处理程序可以从数据库中加载已登录用户并将其保存到g.user中，随后调用视图函数时，视图函数就可以直接使用g.user获取用户。1234567891011121314151617181920212223242526272829from flask import Flask,g,request,render_templat,gdef login_log(): print ('当前登录用户是：%s' % g.username)def login_ip(): print ('当前登录用户的IP是：%s' % g.ip)app = Flask(__name__)@app.route('/')def hello_world(): return 'Hello World!'@app.route('/login/',methods=['GET', 'POST'])def login(): if request.method == 'GET': return render_template('login.html') else: username = request.form.get('username') password = request.form.get('password') g.username = username g.ip = password login_log() login_ip() return '恭喜登录成功！'if __name__ == '__main__': app.run() 程序上下文(app context)current_appcurrent_app，顾名思义这个变量表示当前激活的程序实例1234567from flask import current_app,Flaskapp = Flask(__name__)@app.route('/test')def index(): return current_app.name session用户回话，用于存储请求直接需要“记住”的值的字典，其操作方式也与字典相似。当客户端关闭则session失效。123456789101112131415161718192021from flask import Flask,session,url_forapp = Flask(__name__)@app.route('/login',methods=['GET','POST'])def login(): if request.method == 'POST': session['username'] = request.form.get('username') return url_for('index') return """ &lt;form action="" method="post"&gt; &lt;p&gt;&lt;input type=text name=username&gt; &lt;p&gt;&lt;input type=submit value=Login&gt; &lt;/form&gt;"""@app.route('/')def index(): if 'username' in session: return '%s 已经登录' % session['username'] return '请登录' 蓝图为了方便开发和调试，我们用蓝图将程序分成一些不同的部分。每一部分相互独立。通常同一部分的程序url都带有相同的前缀，如:1http://127.0.0.1/user/1和http://127.0.0.1/2 创建蓝图在flask中，通过自带的blueprint函数就能注册一个创建一个蓝图：123456# !E:/web_develop/bp/__init__.pyfrom flask import blueprintbp = blueprint('bp',__name__)from . import views,errors blueprint接受两个参数：第一个参数是这个蓝图的名字，第二个参数是蓝图所在文件或模块的名字，通常是name注意：我们在创建蓝图后导入bp的视图函数模块views和错误处理模块errors，这是因为这两个模块中都有这一句：1from . import bp 这句话会从init.py中尝试导入bp，如果在创建bp前导入这两个模块，会出现循环调用的问题。 注册蓝图蓝图创建好了之后，还要在分发函数中注册它：123456789# !E:/web_develop/__init__.pyfrom flask import Flaskfrom .bp import bpdef create_app(): app = Flask(__name__) app.config.from_object('config') app.register_blueprint(bp,url_prefix='/bp') ... 通过register_blueprint()函数就可以注册这个蓝图，url_prefix属性指定url前缀。 在蓝图中创建视图函数在蓝图外，我们通常用:@app.route()注册视图函数，在蓝图内，我们用:@蓝图名.route()注册视图函数。如：1234567891011@bp.before_app_requestdef hello(): print ('hello')@bp.errorhandler(404)def error(e): return 'Not Found',404@bp.route('/hi')def hi(): return 'hi' 这样注册的函数的作用范围仅限于蓝图内。要让函数变得全局可用，我们要这样写：1234567@bp.app_errorhandler(404)def error(e): return 'Not Found',400@bp.before_app_requestdef hello(): print('hello')]]></content>
      <categories>
        <category>Web开发</category>
      </categories>
      <tags>
        <tag>flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[书单整理]]></title>
    <url>%2F2018%2F03%2F20%2F%E4%B9%A6%E5%8D%95%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[这几天突发奇想，打算把高中毕业以来读过的书整理一遍，做个书单，方便日后查看，也可以给后来者一些参考。因为平时读的书比较杂，所以这份书单里的书也不仅限于一个方向，都是我读过以后觉得还不错的，推荐给大家。 悬疑推理老实说这方面的书我看的不多，印象比较深的就只有东野圭吾写的几本了(～￣▽￣)～ (～￣▽￣)～ 白夜行这个不多说，东野的代表作，基本上东野迷人手一本吧。如果你没看过推理小说，或者以前没有接触过东野圭吾，那么从入手这本书不会有错的~ 解忧杂货店东野另一代表作，在日本和国内分别都有翻拍的电影。建议看完书以后有兴趣的话可以看一下日本翻拍的那部电影，至于国产的嘛….如果你是王俊凯的粉丝可以看看￣ω￣=说回这本书，书里人物的命运相互交织，冥冥中似乎有一张巨大的网把他们联系在一起：为了躲避抓捕慌乱中躲进杂货店的小贼，被情所困的运动员，为未来彷徨的歌手…我常常会想，自己是否也会像书里说的那样，无意之间改变了他人的命运的走向。毕竟人生在世，没有谁是一座孤岛，再特立独行的人也必定会受到别人的影响。 嫌疑人X的献身如果说刚才两本书着力点在人心，这本书则更注重推理。数学天才为了包庇自己爱的人与警方斗智斗勇的故事。滴水不漏的思维，细致入微的推理，整个故事看下来还是很爽的。毕竟推理小说有点类似于复联这种爽片，能做到人文剧情两不误当然最好，如果不能，单是剧情精彩也就足够了。 编程嘿嘿，毕竟这小一年都在学习这方面的知识，所以这方面的书看得会多些￣ω￣=学习过程中踩过不少坑，有些书写的水平实在一般，有时自己又太着急了买了些自己还没办法理解的书，有时看完一本书后不知道下一步该怎么办…所以我会按自己的学习路线来给下面的书排版。不过要说明的是，我目前的学习方向是爬虫+后端开发，推荐的书也是以这两方面的居多。当然，现在网上有很多的教学视频，但是视频内容大多不全面，且对我来说视频学习效率不如看书，如果书上有内容看不懂，可以找些相关的视频来看看，有时候会有奇效。 C primer plusC入门的经典书籍，C是大部分现代编程语言之母，而且学习C的过程中你还会了解到很多关于计算机底层的知识，所以就算你不准备把C作为主力编程语言，起码也要了解C。这本书说实话挺厚的，不过千万不要被它的厚度吓到了，作为入门我们只需要学习它前半部分的内容，后面的算法知识可以等到以后再补。 Python编程-从入门到实战很不错的一本Python入门书，基于Python3（要知道python2在2020年就要停止维护了，所以现在入门的话python3是最佳选择）。基本把python基础语法都讲到了，每一个知识点都附有代码实例，章节末尾还有对应的练习。最可贵的是教学节奏把握的不错，学习的时候不会有很无聊的感觉。书末还有3个不错的实战项目，跟着这本书学下来python也就入门了。读完这本书之后如果想进一步加深对python的了解可以看一下廖雪峰老师的Python教程廖雪峰的python教程。这个教程也是python入门的好资料，不过难度跨越有点大，等有一点python基础之后看效果更好些。 图解HTTP初学HTTP的好书。学习http主要是因为后面的爬虫和web开发都要用到很多http方面的知识。这是一本日本人写的书，风格幽默风趣，最最棒的是书中有大量的插画帮助理解。类似的还有一本书叫《图解TCP/IP》,这里就不单独介绍了。 Python爬虫开发与项目实战既然已经入门了，当然要称热打铁，找些实际项目来练练手啦，爬虫就是一个很好的练手方向，看着数据一条条地被你从网上抓取下来，会很有成就感的~作为爬虫教程这本书写的很不错，从最基础的爬取静态网页开始，到爬取js渲染的动态网站，再到模拟登陆，处理验证码，反爬…还涉及分布式爬虫的内容。后半部分还对目前最火的python爬虫框架——Scrapy进行了讲解。读完这本书一般的网站都难不住你了。 Python核心编程(第三版)Python进阶书籍。有了一定的python开发基础后就可以看这本书了。主要学习其中的正则表达式，线程和进程，数据库编程，为接下来的Web开发做知识储备。 Flask Web开发备受好评的Flask学习书籍。每一章都有git tag，跟着作者一步步地完成一个web网站的开发。注意：学这本书一定要配合git使用，一步一步的跟着作者敲代码。如果不知道什么是git的可以先去看下廖雪峰的git教程廖雪峰的git教程。这本书写的很详细，学完基本就对flask比较熟悉了。想要了解更多可以看一下flask的文档，推荐看英文的Flask英文文档,如果英语比较差可以看下中文的Flask中文文档，不过最好还是看原文，毕竟翻译作品有时候难以表达作者原意。 数据结构与算法-python语言表述这是我最近在看的一本书，本来没看完的书不该推的，但是目前为止感觉很不错，所以也放上来了。。。本书由北大一名老教师编写，所以有一股很浓厚的国内教科书的气息(￣ー￣)不过讲的很详细，也比较好理解，关键是没有《算法导论》那么厚，比较容易让人接受。另一个原因是它是用python作为范例语言讲的，比较方便学习，哈哈未完待续。。。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>读书体会</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一点感想]]></title>
    <url>%2F2018%2F03%2F08%2F%E4%B8%80%E7%82%B9%E6%84%9F%E6%83%B3%2F</url>
    <content type="text"><![CDATA[今天开了个学情分析会，说实话我觉得很无聊。本来大学里的学习就不应该再像中学似的一切绕着成绩转，当然不是说成绩就不重要了，只是我想到了现在更重要的是磨炼自己的技能，增长见识。其实以前也好，现在也好，成绩讲究的是一个够用，这个标准自然因人而异：考二本的400+够用，要考清北的670+够用，想要混过4年的不挂科就够用，想要保研的3.5+才够用……对我而言，上学期的成绩是否够用还是个未知数，一切还要再等3个月才能知晓。不过不管是不是够用，大概都是有一个遗憾的：最重视的数学和英语学的并不好，这点倒是令我挺难受。其他的课程学的好也罢，差也罢，我并不是很在乎，唯独这两门我是没办法做到不在意的。还有就是写代码这件事了。接触这行也是机缘巧合。后来学得多了些，慢慢地也开始体会到乐趣所在，现在是CS的黄金年代，大量的人一拥而入，这是很正常的。但是不得不说如今的CS已经积累到了量变的程度，人工智能,区块链，云计算……这些方向都是多学科交叉的，统计学，数学，计算机科学，甚至物理学…以前那种能写些网站和app，会调个库，但是对底层一无所知还能被称为程序员的的时代已经快要过去了，未来必定会要求程序员对数据结构，算法，各种协议甚至硬件有更深的理解。路还很长，要努力啊]]></content>
      <categories>
        <category>心得</category>
      </categories>
      <tags>
        <tag>闲聊</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《python核心编程》知识整理之正则表达式]]></title>
    <url>%2F2018%2F03%2F08%2F%E3%80%8Apython%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B%E3%80%8B%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%E4%B9%8B%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[特殊符号和字符正则表达式由多个字符组成，而有些字符在正则表达式中有特殊的含义，这部分字符被称为元字符。正是有了它们正则表达式才会如此强大。下面对一些常用字符做介绍。 择一匹配符：||&nbsp;起一个或的作用。如：12re.search(r'hello|hi','hello,jack.').group() # 匹配到hellore.search(r'hello|hi','hi,jack').group() # 匹配到hi 匹配任意单个字符：.. &nbsp;会匹配除了换行符\n和空字符以外的任意字符。1re.search(r'.end','end').group() # 匹配失败，因为匹配目标end的开头是一个空字符 从字符串的起始/结尾或者单词边界开始匹配：^,$,\b,\B^ &nbsp;会匹配字符串的起始部分1re.search(r'^From','From China') # 这样会匹配所有以From开始的字符串 $ &nbsp;会匹配字符串的末尾部分1re.search(r'jack$','hello,jack') # 这样会匹配所以以jack结尾的字符串 \b,\B 创建字符集：[ ]在[&nbsp; ]里面的字符相当于被多个或运算符相连。[0-9]会匹配0到9的任意一个数字，[a-z]会匹配a到z的任意一个字母。[^…]表示不匹配该字符集内的任意一个字符。如[^A-Z]表示不匹配所有的大写字母。 闭包操作符和频数匹配：*，+，？，{}闭包操作符都是对其左边的字符进行操作。* &nbsp;代表对其左边的正则表达式进行0次或者多次匹配。+ &nbsp;代表对其左边的正则表达式进行1次或多次匹配。? &nbsp;代表对其左边的正则表达式进行0次或者1次匹配。{M,N} {M}代表对其左边的正则表达式进行M到N次/M次匹配.以上都称作闭包操作符。 贪心模式和非贪心模式正则表达式在匹配时，尽可能多的进行匹配。比如说对于一个匹配目标，如果+既可以匹配1次，也可以匹配多次，正则表达式会优先匹配多次。这就是贪心模式。而在闭包操作符后面加上?就可以进入非贪心模式，按尽可能少的次数进行匹配。12345data = 1111aa1re.search(r&apos;\w+(\d)&apos;).group(1) # 这样是无法取到分组1中的匹配对象的，因为\w+就已经把全部字符串匹配完毕。re.search(r&apos;\w+?(\d)&apos;).group(1)# 这样分组1中的匹配对象就是1，对应data中的第二个1。 使用圆括号进行分组像(\d+)-(\d+)这样的表达式会匹配形如666-888的字符串。用圆括号对表达式进行分组有利于以后使用子组的匹配值。要使用子组内的值，只需要\N的方式就能调用。如12re.sub(r'(\d+)-(\d+)','\2\1','666-888')# \2会调用第2个子组里的匹配值，\1会调用第一个子组里的匹配值。输出结果为888-666 Pythonre模块的使用 搜索search()与匹配match()不同点： 搜索search()会在字符串的任意位置搜索匹配的模式。 匹配match()是判断一个字符串能否从起始处全部或者部分的匹配某个模式。 共同点： 二者都会返回一个匹配对象。匹配对象当处理正则表达式时，除了正则表达式对象外，还有另外一种对象类型：匹配对象。这是成功调用match()或search()返回的对象。匹配对象有两个主要的方法：group()和groups()。group()group()要么返回整个匹配对象，要么根据要求返回特定的子组。1234pat = '(\d+)-(\d+)'data = '888-666'print(re.search(pat,data).group()) # 输出888-666,因为此时整个匹配对象是888-666print(re.search(pat,data).group(2)) # 输出666，因为此时group()返回子组2的匹配对象 groups()groups()仅返回一个包含唯一或全部子组的元组，如果没有子组，那么会返回一个空元组。12345678pat1 = '(th\w+).*?and (th\w+).*'pat2 = '(this is (a(pple))) and that is (banana)'pat3 = '\w+'data = 'this is apple and that is banana'print(re.search(pat1,data).groups()) # 输出('this','that')print(re.search(pat2,data).groups()) # 输出('this is apple', 'apple', 'pple', 'banana')print(re.search(pat3,data).groups()) # 输出(),因为没有子组 使用sub()与subn()搜索与替换有两个函数可以实现搜索与替换：sub()与subn()，两个几乎一样，都是将字符串中所有匹配正则表达式的部分进行某种形式的替换，用来替换的部分通常是一个字符串，可也以是一个返回字符串的函数。subn()和sub()一样，但是subn()还返回一个表示替换的总数，替换后的字符串和替换总数一起作为一个拥有两个元素的元组返回。12345678pat1 = 'hello'pat2 = '(hello),(jack)'repl = 'hi'data = 'hello,jack'print(re.sub(pat1,repl,data)) # 输出hi,jackprint(re.subn(pat1,repl,data)) # 输出('hi,jack',1)print(re.sub(pat2,r'\2,\1',data)) # 输出jack,hello(用上面提到的引用子组匹配对象的方法进行替换) 使用split()分隔字符串re模块和正则表达式的对象方法split()对于相对应字符串的工作方式是类似的，但是与分割一个固定字符串相比，它们基于正则表达式的模式分隔字符串，为字符串的分隔功能添加一些额外的威力。如果给定分隔符不是使用特殊符号来匹配多重模式的正则表达式，那么re.split()与str.split()的工作方式相同。如果不想为每次模式的出现都分隔字符串，可以通过为maxsplit参数设定一个值(非0)来指定最大分割数。1234567data = '123:abc:def:456:789'pat = '\d+:'print(re.split(r':',data)) # 输出['123', 'abc', 'def', '456', '789']print(data.split(':')) # 输出['123', 'abc', 'def', '456', '789']print(re.split(pat,data)) # 输出['', 'abc:def:', '789']print(re.split(pat,data,maxsplit=1)) # 输出['', 'abc:def:456:789']]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>读书体会</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫之爬取静态网页（实战篇）]]></title>
    <url>%2F2018%2F02%2F24%2Fpython%E7%88%AC%E8%99%AB%E4%B9%8B%E7%88%AC%E5%8F%96%E9%9D%99%E6%80%81%E7%BD%91%E9%A1%B5%E5%AE%9E%E6%88%98%E7%AF%87%2F</url>
    <content type="text"><![CDATA[以爬取桌酷网下桌面最新壁纸一栏中的图片为例。本篇教程旨在让新手快速入门爬虫，所以不涉及面向对象编程。12345678910111213141516# encoding:utf-8import requestsfrom bs4 import BeautifulSoupimport osfrom random import choicefrom time import sleepurl = "http://www.zhuoku.com/"header_list = [ "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.91 Safari/537.36", "Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50", "Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)", "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; 360SE)"]header1 = &#123;"User-Agent":"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)"&#125;header1 = &#123;"User-Agent":"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)"&#125; 先导入必要的库，配置该项爬虫的基本设置。在header_list中存放几个User-Agent反爬。123456try: r = requests.get(url,headers=header1,timeout=30)except: r = requests.get(url,headers=header1,timeout=30)r.encoding = 'utf-8' 利用requests.get获取桌酷网首页的html文件，并将其设置为utf-8格式。timeout设置为30秒，如果30秒内没有获取成功会报错，为了避免连接超时导致程序停止在except中让程序再次连接这个url12soup = BeautifulSoup(res,"html.parser")all_a = soup.find("div",id="zuixin").find_all('a',attrs=&#123;"class":"title"&#125;) 煮一锅“汤”，解析下载好的html文件。在文件中找到‘zuixin’下所有分类为‘title’的a标签。12345for a in all_a: header = &#123;"User-Agent": choice(header_list)&#125; title = a.get_text().replace("/",'') href = a.get("href") img_url = url + href[1:-4] + "(1).htm"#补第一张图片的全href 值得注意的是，此时我们用choice()函数从上面设置好的header_list列表中随机选择了一个User-Agent作为header里的内容。在下面的每一次requests.get()中都会加上这个header，这样就可以突破很多网站的反爬措施了。遍历找出的每一个a标签,通过get_text()方法取出该标签的元素作为这组图片的title，通过get(‘href’)取出该a标签的href属性内容。通过观察可以发现，这组图片的url就是网站首页url与取出的href属性拼接而成，后面(1).htm这个后缀实际上代表了这组图片的页数。(1).htm既是这组图的第一张图片，(2).htm既是这组图的第二张图片…通过补全url构造出每一组图的第一张图片url。12345678910111213if os.path.isdir(os.path.join("D:\zhuoku",title)): #如果存在文件夹 print("exist" + title) passelse: os.makedirs(os.path.join("D:\zhuoku",title)) #创建文件夹 print("makedir" + title)os.chdir("D:\zhuoku\\" + title) #切换到此文件夹try: img_url_get = requests.get(img_url,headers=header,timeout=30)except: img_url_get = requests.get(img_url,headers=header,timeout=30)sleep(0.5) 判断一下储存图片的文件夹是否存在，不存在就创建一个（os.join()将两个路径拼接起来），然后通过os.chdir()移动到这个文件夹下。还是通过requests.get()获取这张图片页面的html文件。sleep()让程序停止0.5秒，免得访问太频繁给服务器带来太大的负担。设置合适的sleep()时间很重要，既是出于不给网站带来太大压力的道德因素，也是避免访问过于频繁导致服务器强制断开连接使程序中断1234567891011img_url_soup = BeautifulSoup(img_url_get.text,"html.parser")# 找出最大页数max_img_page = img_url_soup.find('div',id="yema").find_all("a")[-1].get_text()for page in range(1,int(max_img_page)+1): jpg_href = url + href[1:-4] + "(" + str(page) + ").htm" + "#turn" try: jpg_href_get = requests.get(jpg_href,headers=header,timeout=30) except: jpg_href_get = requests.get(jpg_href,headers=header,timeout=30) sleep(0.5) 解析下载好的html文件，在html文件中找到这组图片的最大页数。依次构造这组图的每一张图片的url。通过1234jpg_soup = BeautifulSoup(jpg_href_get.text,"html.parser")# 在find()方法后面用方括号将属性括起来可以取出该属性的值jpg_url = jpg_soup.find("div",id="bizhiimg").find("img")["src"] name = jpg_url[-9:] #截取倒数第九位至末尾为图片的名字 依次解析每一张图片的html文件，通过find(‘img’)找出图片所在的img标签并取出src属性的值（类似于get(‘src’)），这个值就是图片URI。12345678910111213141516171819 if os.path.isfile(name): #如果存在名为name的文件 print(name + " exist skip") pass #下面全跳过 else: jpg_header = &#123; "Referer": jpg_href, "User-Agent":choice(header_list) &#125; try: jpg = requests.get(jpg_url,headers=jpg_header,timeout=30) except: jpg = requests.get(jpg_url,headers=header,timeout=30) sleep(0.5) with open(name,'wb') as f: f.write(jpg.content) print(name+" saved")print("congratulations! all finished!") 判断一下图片是否已经保存。如果没有保存通过requests.get()访问刚才找出的URI，此时变量jpg就可以看做是这张图片。以‘wb’形式新建一个名为name变量的值的文件，通过f.write(jpg.content)将图片写入文件中。成功！现在你就有一个完整的爬虫了！启动程序，大概20分钟之后，这些图片就会统统下载到你的电脑里啦~以下是程序的完整代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273# encoding:utf-8import requestsfrom bs4 import BeautifulSoupimport osfrom random import choicefrom time import sleepurl = "http://www.zhuoku.com/"header_list = [ "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.91 Safari/537.36", "Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50", "Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)", "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; 360SE)"]header1 = &#123;"User-Agent":"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)"&#125;try: r = requests.get(url,headers=header1,timeout=30)except: r = requests.get(url,headers=header1,timeout=30)r.encoding = 'utf-8'soup = BeautifulSoup(r,"html.parser")all_a = soup.find("div",id="zuixin").find_all('a',attrs=&#123;"class":"title"&#125;)for a in all_a: header = &#123;"User-Agent": choice(header_list)&#125; title = a.get_text().replace("/",'') href = a.get("href") img_url = url + href[1:-4] + "(1).htm"#补第一张图片的全href if os.path.isdir(os.path.join("D:\zhuoku",title)): #如果存在文件夹 print("exist" + title) pass else: os.makedirs(os.path.join("D:\zhuoku",title)) #创建文件夹 print("makedir" + title) os.chdir("D:\zhuoku\\" + title) #切换到此文件夹 try: img_url_get = requests.get(img_url,headers=header,timeout=30) except: img_url_get = requests.get(img_url,headers=header,timeout=30) sleep(0.5) img_url_soup = BeautifulSoup(img_url_get.text,"html.parser") max_img_page = img_url_soup.find('div',id="yema").find_all("a")[-1].get_text() for page in range(1,int(max_img_page)+1): jpg_href = url + href[1:-4] + "(" + str(page) + ").htm" + "#turn" try: jpg_href_get = requests.get(jpg_href,headers=header,timeout=30) except: jpg_href_get = requests.get(jpg_href,headers=header,timeout=30) sleep(0.5) jpg_soup = BeautifulSoup(jpg_href_get.text,"html.parser") jpg_url = jpg_soup.find("div",id="bizhiimg").find("img")["src"] #在find方法后面用方括号将属性括起来可以取出该属性的值 name = jpg_url[-9:] #截取倒数第九位至末尾为图片的名字 if os.path.isfile(name): #如果存在名为name的文件 print(name + " exist skip") pass #下面全跳过 else: jpg_header = &#123; "Referer": jpg_href, "User-Agent":choice(header_list) &#125; try: jpg = requests.get(jpg_url,headers=jpg_header,timeout=30) except: jpg = requests.get(jpg_url,headers=header,timeout=30) sleep(0.5) with open(name,'wb') as f: f.write(jpg.content) print(name+" saved")print("congratulations! all finished!")]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flask-sqlalchemy框架的使用]]></title>
    <url>%2F2018%2F02%2F13%2Fsqlalchemy%E6%A1%86%E6%9E%B6%E4%B8%8B%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93%E5%85%B3%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[关系型数据库中表的关系一对多考虑下面的代码12345678910111213141516171819202122232425from flask-sqlalchemy import SQLAlchemyfrom flask import Flaskimport osapp = Flask(__name__)# 配置app，实例化SQLAlchemybasedir = os.path.abspath(os.path.dirname(__file__))app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///' + os.path.join(basedir + 'db.sqlite')app.config['SECRECT_KEY'] = 'a secrect string'db = SQLAlchemy(app)# 定义模型class Writer(db.Model): __tablename__ = writers id = db.Column(db.Integer,primary_key=True) name = db.Column(db.String(64),unique=True) posts = db.relationship('Article',backref='writer')class Article(db.Model): __tablename__ = articles id = db.Column(db.Integer,primary_key) title = db.Column(db.String(64)) body = db.Column(db.String(2018)) writer_id = db.Colum(db.Integer,db.ForeignKey('writers.id')) 在Writer模型中，relationship()定义了两张表之间的关系：第一个参数是子表模型名，backref属性相当于在另一张表中也定义了一个相关关系，用于访问父表的模型。这样Writer实例通过posts属性可以访问所有与之相关的Article模型，返回一个关联Article组成的列表。Article实例通过writer属性可以访问对应的Writer模型。 在Article模型中，writer_id属性被定义为外键，ForeignKey()函数的含义是其所在的列的值域应当被限制在另一个表的指定列的取值范围之内。这里要说明:ForeignKey()函数的参数形式应为’表名.字段名’而不是’Model名.字段名’ 一对多关系中，在父表模型中定义db.relationship(),用于指出和子表的关系，在子表模型中定义db.ForeignKey指向父表。 如下例,在实例化Article模型时，由relationship反向定义的writer属性要传入与love_python对应的的Writer实例susan。这样就可以让love_python与susan两个实例关联起来。12susan = Writer(name=susan)love_python = Article(title='love python',body='python is easy and elegant'，writer=susan) 关系型数据库是通过主键与外键确定两张表的关系的。比如12345678w1 = Writer('Jack')w2 = Writer('Mike')a = Article('hello','world',writer=w1)、b = Article('hi','i like python',writer=w1)c = Article('haha','i like flask',writer=w2)db.session.add_all([w1,w2,a,b,c])db.session.commit()print(w.posts) # 会访问a和b，而不访问c 当w查询posts时，sqlalchemy会从Article模型中寻找外键与w.id相同的实例并返回这些对象。 多对一多对一关系中模型的定义与一对多类似，但是要将ForeignKey定义在父表中，即多的一方。123456789101112class Writer(db.Model): __tablename__ = writers id = db.Column(db.Integer,primary_key=True) name = db.Column(db.String(64),unique=True) article_id = db.Column(db.Integer,db.ForeignKey(articles.id) posts = db.relationship('Article',backref='writer')class Article(db.model): __tablename__ = articles id = db.Column(db.Integer,primary_key=True) title = db.Column(db.String(64)) body = db.Column(db.String(2018)) 一对一要让两张表是一对一关系，定义模型方式类似于一对多。只需要在一的模型中其relationship()的uselist参数设为False。对于多对一关系，要改成一对一关系也很简单，只要用backref()函数在一的一方定义一个关系，并且将urslist设为False即可。123456class Writer(db.Model): __tablename__ = writers id = db.Column(db.Integer,primary_key=True) name = db.Column(db.String(64),unique=True) article_id = db.Column(db.Integer,db.ForeignKey(articles.id) posts = db.relationship('Article',backref=db.backref('writer',uselist=False) 多对多要定义两张表的多对多关系，这时候光用两张表就不够了。要引入第三张表。举个例子，现在要定义课程与学生之间的关系。由于一个课程对应多个学生，一个学生也对应对个课程，这时候就不再是简单的一对多或者多对一而是多对多关系了。为了解决这个问题，我们引入第三张表Reflection，这张表定义了学生的id，对应的课程id与学生选这门课程的时间。这样一来，如果我们想要知道小明选了什么课，只需要在Reflection中根据小明的id找出对应的课程id，再通过课程id在Class中找到对应课程就OK啦~同时，通过Reflection我们还可以知道小明在什么时候选了这门课。12345678910111213141516171819class Reflection(db.Model): __tablename__ = 'reflections' id = db.Column(db.Integer,primary_key=True) student_id = db.Column(db.Integer,db.ForeignKey('students.id')) class_id = db.Column(db.Integer,db.ForeignKey('classes.id')) timestamp = db.Column(db.DateTime,default=datetime.utcnow)class Student(db.Model): __tablename__ = 'students' id = db.Column(db.Integer,primary_key=True) name = db.Column(db.String(64)) age = db.Column(db.Integer) classes = db.relationship('Reflection',backref=db.backref('student',lazy='joined'),lazy='dynamic')class Class(db.Model): __tablename__ = 'classes' id = db.Column(db.Integer,primary_key=True) name = db.Column(db.String(64)) student = db.relationship('Reflection',backref=db.backref('class',lazy='joined'),lazy='dynamic') 值得一提的是，在定义Reflection中回引模型的属性时用了backref()方法，并且将回引属性定义为joined加载。joined加载会在调用Reflection的同时找出对应的Student模型和Class模型，换言之，此时Reflection中的student和class直接指向对应的实例。这样就避免了selected加载导致的仅在调用Reflection.student或Reflection.class才加载相应模型。因为从数据库中加载模型是很耗费时间的，用joined一次就把所有模型都调用出来了，而selected需要调用多次，换言之，这样提高了效率。定义好的模型的关系，我们就可以试着进行操作了：先创建学生和课程实例12345Mike = Student(name='Mike',age=18)English = Class(name='English')db.session.add(Mike)db.session.add(English)db.session.commit() 将学生与课程之间的关系添加到Reflection中123f = Reflection(student=Mike,class=English)db.session.add(f)db.session.commit() 当我们想要查询学生的课程时，通过Student.classes获取到Reflection对象，再通过Reflection.class就可以查到该学生的课了。12student = Student.query.filter_by(name='Mike').first()classes = student.classes.class.all() 联结查询在上面的例子中，我们为了获取学生的课程，执行了多次查询，这样的效率太低了，最好是一次就直接把结果查询出来。我们可以考虑把Class和Reflection结合起来，然后分别过滤Reflection中student_id与class_id，这样就得到一样包含了学生和相应课程的表。这种操作就叫做联结查询。12345class Student(db.Model): ...... @property def getclass(self): return Reflection.query.join(Class,Class.id==Reflection.class_id).filter(Reflection.student_id==self.id) 下面对这行代码进行分析： Reflection.query返回Reflection对象 join(Class,Class.id==Reflection.class_id) 联结Class与Reflection对象，并且将Class.id与Reflection.class_id的值对应起来 filter(Reflection.student_id==self.id) 对这张临时表进行过滤：只有Reflection.student_id与当前学生实例id相同的会留下来 这样，我们就可以用1Mike.getclass 获得Mike的课程了。]]></content>
      <categories>
        <category>Web开发</category>
      </categories>
      <tags>
        <tag>flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python杂谈]]></title>
    <url>%2F2018%2F02%2F07%2Fpython%E6%9D%82%E8%B0%88%2F</url>
    <content type="text"><![CDATA[面向对象编程（OOP）方法与函数面向对象编程思想中，方法是指一个对象可以使用的函数，举个例子12arr = [1,2,3]arr.remove(2) arr被定义为指向列表对象的变量，而remove则是list对象的一个方法，用于从列表中删除某个值。而函数则是对对象进行操作，例如12arr = [1,2,3]len(arr) len()函数对其参数求长度。在本例中len()的参数既是一个列表。 一切皆对象python一个很出名的特性是一切皆对象。比如变量，类，甚至函数也可以作为一个对象传给变量。比如：12345678910def plus(x,y): return x+yplus(1,3)from threading import Threatif __name__ == '__main__': t = Threat(target=plus,args=(1,3,)) t.start() t.join() 第八行的target参数传入的是plus函数的对象，此时不需要括号。而第三行的plus()加了括号是告诉python解释器执行这个函数对象。 python常用的装饰器python中的装饰器可以对函数进行扩展。 @property@property装饰器可以把类的方法变成属性。例如1234567class Student(): def __init__(self,score): self.score = score#调用Student类qiuyue = Student(90)print(qiuyue.score) #输出结果90 可是这样也不无问题，比如当输入分数不合理（如1000）时，无法对分数进行检查。当然，可以对Student类附加方法实现检查分数。123456789101112131415class Student(): def get_score(self): return self._score def set_score(self,value): if not isinstance(score,int): raise ValueError('score must is an integer!') if value &gt; 100 or value &lt; 0: raise ValueError('score must between 0 to 100!') self._score = value#调用Student类qiuyue = Student()qiuyue.set_score(90)print(qiuyue.get_score()) #输出90 通过set方法对Student的score属性赋值，再用get方法获取score属性。这样就可以实现对score合法性的检查。但是为了一个属性特地写两个方法未免过于繁琐。所以用到装饰器@property封装set方法和get方法，实现对score属性赋值的同时进行数值合法性检查。 12345678910111213141516171819202122232425262728293031class Student(): @property def score(self) return self._score @score.setter def score(self,value) if not isinstance(value,int): raise ValueError('score must is an integer!') if value &gt; 100 or value &lt; 0: raise ValueError('score must between 0 to 100!') self._score = value @property def grade(self): if self._score is None: return None elif self._score &gt;= 90: print('优秀！') elif self._score &gt;=60: print('及格！) else: print('不及格！')#调用Student类qiuyue = Student()qiuyue.score = 90print(qiuyue.score) # 输出90qiuyue.score = 1000 # 报错，ValueErrorqiuyue.grade = '及格' # 报错qiuyue.grade # 输出优秀 可以看到，@property装饰的第一个score，实际上是一个get方法，而@score.setter装饰的第二个score实际上是set方法。@score.setter其实是@property装饰器的副产品。这两个装饰器一个装饰get方法，一个装饰set方法，这样就使score方法变成了Student类的属性，在对score属性赋值（即set方法）时会自动对值的合法性进行检查，调用score属性即调用get方法。@grade.setter并不是必须的，当缺少@grade.setter装饰器时grade属性变成只读属性，无法对其进行赋值，只能读取。 @classmethod与@staticmethod在介绍类方法@classmethod与静态方法staticmethod前，先要清楚一个概念：类属性与实例属性是不同的。12345678class Plus(): num = 1f = Plus()f.num = 2print(Plus.num) # 结果为2 print(f.num) # 结果为1 众所周知，要调用类的方法，我们首先要把这个类实例化，然后通过实例名.方法名的方式调用。而@classmethod与@staticmethod都可以实现通过类名.方法名的方式调用方法。 @classmethod当类中有些方法不需要涉及实例，而需要涉及类，如对类属性的修改，往往使用@classmethod。用@classmethod修饰的方法不会将实例传入方法中，而会自动将自身类作为第一个参数传入。所以这个方法不需要写self参数，但需要一个cls参数代表这个类。12345678class Apple(): apple = 1 @classmethod def how_much(cls): if cls.app == 1: print('还有一个苹果')Apple.how_much() # 输出：还有一个苹果 @staticmethod如果类中有些方法既不涉及类，也不涉及实例，可以用@staticmethod。@staticmethod既不会将实例传入方法，也不会将自身类传入方法。所以既没有self参数也没有cls参数。12345678910111213class Apple(): apple = 1 def change(self,data): self.apple = data print('还有%s个苹果' % self.apple) @staticmethod def how_much(): print('没有苹果了')apple = Apple()apple.change(2) # 输出：还有2个苹果Apple.how_much() # 输出： 没有苹果了 下面这个例子加深区分：1234567891011121314151617181920class Apple(): num = 1 def __init__(self,data): self.num = data def common(self): print('还有%s个苹果' % self.num) @classmethod def clsmed(cls): print('还有%s个苹果' % cls.num) @staticmethod def stamed(): print('没有苹果了')apple = Apple(2)apple.common() # 输出：还有2个苹果Apple.clsmed() # 输出：还有1个苹果Apple.stamed() # 输出：没有苹果了 Python的特点像java，C#这种编译型语言，会将代码编译成二进制再运行。而python作为一种解释型语言，是动态的逐行解释代码的，也就是从脚本第一行开始，没有统一的入口。一个Python源码文件除了可以被直接执行外，还可以作为模块（也就是库）被其他.py文件导入。此时这个源码文件的文件名（不包括.py）就是库名。python本身有很多有趣的方法，会在每一个python文件里自动生成，在特殊情况下还会自动调用，这种方法称之为魔法方法。魔法方法的形式为两个下划线(__)+方法名+两个下划线(__)。如：__new__。下面介绍一些常见的魔法方法。 __file__通过下面一行代码，就能很直接地看出__file__的作用。123# 文件位置为E:/python/test.pyprint(__file__)# 输出E:/python/test.py 可见，__file__代表了当前python文件的路径。而且如前面所言，这个方法是python自动实现的，不需要你去编写。 __name__相信不少python初学者都见到过这样一段代码：12if __name__ == "__main__": app.run() 可能很多人第一次看到这段代码的时候都会困惑：这个__main__我理解，是主函数的意思，可是这__name__是个什么东东？老规矩，上一段代码:123# 文件位置为E:/python/test.pyprint(__name__)# 输出__main__ java，C等语言都会显示地定义一个main()函数，一个用C编写的程序都是以main()作为程序入口的。而python不同，哪个文件被直接执行，哪个文件的模块名就是__main__。现在说回__name__, __name__存放的就是当前python文件的名字，那么现在情况就很明显了，开头那段代码的意思是：如果这个文件是被直接运行的，就执行app.run()，如果这个文件是被别的文件导入后运行的，就会跳过app.run()。这样做的好处是避免了一些只能在主程序里执行的代码由于被导入了其他文件而错误执行。 requirements.txt在查看别人的python项目时，常会见到一个requirements.txt文件，主要是说明这个项目依赖的模块及其版本。我们可以用这个命令生成requirements.txt文件：1pip freeze &gt; requirements.txt 要导入这个文件里指出的模块也很简单，只要用:1pip install -r requirements.txt]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>杂谈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫之爬取静态网页(知识储备篇)]]></title>
    <url>%2F2018%2F01%2F29%2Fpython%E7%88%AC%E8%99%AB%E4%B9%8B%E7%88%AC%E5%8F%96%E9%9D%99%E6%80%81%E7%BD%91%E9%A1%B5%2F</url>
    <content type="text"><![CDATA[断断续续地学习爬虫也有一段时间了。最近接到点石的通知在寒假会开始Web后端方向的学习，趁着这几天有空抓紧把已经学会的知识整理一下。 HTTP相关知识客户端与服务器之间通过TCP/IP等协议进行HTTP报文的传输。HTTP报文又分为请求报文（客户端发送至服务器）和响应报文（服务器发送至客户端）。无论是哪种报文，其格式都是大同小异的。下面进行通过《图解HTTP》上的一张图片进行简单介绍上图分别是一个请求报文和一个响应报文。请求报文又分成两部分：请求头（GET/HTTP/1.1）和请求首部（XXXX:XXXX）。get是一种请求方法，表示请求从服务器获得信息，除此之外还有post方法（从服务器获得信息的同时向服务器传递一些信息），put方法（向服务器传递信息），delect方法（从服务器删除信息）等。HTTP/1.1表示该请求使用的是1.1版本的HTTP协议。而首部字段则包含了本次请求的有关信息，如Host表示本次请求的主机地址，User-Agent表示发出请求的浏览器内核信息，Accept-luangage表示浏览器接受的语言等。响应报文的响应头是HTTP/1.1 200 OK。200是一种状态码，表示成功，4xx表示客户端错误，5xx表示服务器错误，3xx表示重定向。响应首部字段内容与请求首部作业相同，报文主体里则包含了本次响应的具体信息，通常是HTML文档。更多有关http知识可以看《图解http》这本书，有大量的图例，内容较为浅显，适合新手入门。 requests库相关知识requests库是python爬虫常用库,常用于下载网页的html代码。使用上相比urllib要方便快捷得多。可以从官方文档获取有关这个库的详细使用方法requests文档 BeautifulSoup库相关知识BeautifulSoup库常用于解析下载好的HTML，通常与requests库配合使用。可以通过官方文档学习有关知识BeautifulSoup文档值得一提的是其自带了html_parser解析器，也可以在cmd下通过 pip isntall lxml 安装lxml解析器，由于lxml是用C写的，所以解析速度比html_parser快，但是用lxml作为解析器有时候会出现解析出来的HTML文档部分丢失的情况。 HTML相关知识HTML是网页的骨架，js，css都要有在HTML的基础才能进行开发。即使是Web后端也要对HTML有简单的认识。对于这部分的知识可以去W3school进行学习，里面还有关于JavaScript的教程，也是知识体系中很重要的一环，可以一并学习。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进程与线程]]></title>
    <url>%2F2018%2F01%2F09%2F%E8%BF%9B%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[什么是进程/线程众所周知，CPU是计算机的核心，它承担了所有的计算任务。而操作系统是计算机的管理者，是一个大管家，它负责任务的调度，资源的分配和管理，统领整个计算机硬件。应用应用程序是具有某种功能的程序，程序运行与操作系统之上。 进程进程时一个具有一定功能的程序在一个数据集上的一次动态执行过程。进程由程序，数据集合和进程控制块三部分组成。程序用于描述进程要完成的功能，是控制进程执行的指令集；数据集合是程序在执行时需要的数据和工作区；程序控制块（PCB）包含程序的描述信息和控制信息，是进程存在的唯一标志。 线程在很早的时候计算机并没有线程这个概念，但是随着时代的发展，只用进程来处理程序出现很多的不足。如当一个进程堵塞时，整个程序会停止在堵塞处，并且如果频繁的切换进程，会浪费系统资源。所以线程出现了。线程是能拥有资源和独立运行的最小单位，也是程序执行的最小单位。一个进程可以拥有多个线程，而且属于同一个进程的多个线程间会共享该进行的资源。 进程与线程的区别 一个进程由一个或者多个线程组成，线程是一个进程中代码的不同执行路线。 切换进程需要的资源比切换线程的要多的多。 进程之间相互独立，而同一个进程下的线程共享程序的内存空间（如代码段，数据集，堆栈等）。某进程内的线程在其他进程不可见。换言之，线程共享同一片内存空间，而进程各有独立的内存空间。以下是作者在知乎上看到的关于进程与线程的讨论，其中一个作者感觉很有道理，摘抄如下： 作者：zhonyong链接：https://www.zhihu.com/question/25532384/answer/81152571来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 首先来一句概括的总论：进程和线程都是一个时间段的描述，是CPU工作时间段的描述。下面细说背景：CPU+RAM+各种资源（比如显卡，光驱，键盘，GPS, 等等外设）构成我们的电脑，但是电脑的运行，实际就是CPU和相关寄存器以及RAM之间的事情。一个最最基础的事实：CPU太快，太快，太快了，寄存器仅仅能够追的上他的脚步，RAM和别的挂在各总线上的设备完全是望其项背。那当多个任务要执行的时候怎么办呢？轮流着来?或者谁优先级高谁来？不管怎么样的策略，一句话就是在CPU看来就是轮流着来。一个必须知道的事实：执行一段程序代码，实现一个功能的过程介绍 ，当得到CPU的时候，相关的资源必须也已经就位，就是显卡啊，GPS啊什么的必须就位，然后CPU开始执行。这里除了CPU以外所有的就构成了这个程序的执行环境，也就是我们所定义的程序上下文。当这个程序执行完了，或者分配给他的CPU执行时间用完了，那它就要被切换出去，等待下一次CPU的临幸。在被切换出去的最后一步工作就是保存程序上下文，因为这个是下次他被CPU临幸的运行环境，必须保存。串联起来的事实：前面讲过在CPU看来所有的任务都是一个一个的轮流执行的，具体的轮流方法就是：先加载程序A的上下文，然后开始执行A，保存程序A的上下文，调入下一个要执行的程序B的程序上下文，然后开始执行B,保存程序B的上下文。。。。========= 重要的东西出现了========进程和线程就是这样的背景出来的，两个名词不过是对应的CPU时间段的描述，名词就是这样的功能。进程就是包换上下文切换的程序执行时间总和 = CPU加载上下文+CPU执行+CPU保存上下文线程是什么呢？进程的颗粒度太大，每次都要有上下的调入，保存，调出。如果我们把进程比喻为一个运行在电脑上的软件，那么一个软件的执行不可能是一条逻辑执行的，必定有多个分支和多个程序段，就好比要实现程序A，实际分成 a，b，c等多个块组合而成。那么这里具体的执行就可能变成：程序A得到CPU =》CPU加载上下文，开始执行程序A的a小段，然后执行A的b小段，然后再执行A的c小段，最后CPU保存A的上下文。这里a，b，c的执行是共享了A的上下文，CPU在执行的时候没有进行上下文切换的。这里的a，b，c就是线程，也就是说线程是共享了进程的上下文环境，的更为细小的CPU时间段。到此全文结束，再一个总结：进程和线程都是一个时间段的描述，是CPU工作时间段的描述，不过是颗粒大小不同。 开进程需要时间学习《python爬虫开发与项目实践》时，执行下面一段代码：1234567891011121314from multiprocessing import Processimport osdef run_process(name): print("Child process %s (%s) is running" % (name,os.getpid()))if __name__ == "__main__": print("parant process %s " % os.getpid()) for i in range(5): p = Process(target=run_process, args=(str(i),)) print("process will start") p.start() p.join() print("process end") 显示的结果是123456789101112parant process 6332 process will startprocess will startprocess will startprocess will startprocess will startChild process 2 (9896) is runningChild process 0 (11208) is runningChild process 3 (5464) is runningChild process 1 (10208) is runningChild process 4 (12596) is runningprocess end 可以看到，程序在执行完1print (&quot;parant process %s &quot; % os.getpid()) 没有接着马上执行run_process()，而是先打印process will start，最后把子进程一起执行。这是因为子进程的创建是需要时间的，在这个空闲时间里父进程继续执行代码，而子进程在创建完成后显示。 Pool进程池需要创建多个进程时，可以使用multiprocessing中的Pool类开进程池。Pool()默认开启数量等于当前cpu核心数的子进程（当然可以手动改变）1234567891011121314from multiprocessing import Pooldef hello(i): print("hello ,this is the %d process" % i)def main(): p = Pool() for i in range(1,5): p.apply_async(target=hell0,args=(i,)) p.close p.joinif __name__ == "__main__": main() apply_async表示在开进程时不阻塞主进程，是异步IO的一种方式之一。targe参数传入要在子线程中执行的函数对象，args以元组的方式传入函数的参数。join会等待线程池中的每一个线程执行完毕，在调用join之前必须要先调用close，close表示不能再向线程池中添加新的process了。 进程间的通信每个进程各自有不同的用户地址空间,任何一个进程的全局变量在另一个进程中都看不到，所以进程之间要交换数据必须通过内核,在内核中开辟一块缓冲区,进程A把数据从用户空间拷到内核缓冲区,进程B再从内核缓冲区把数据读走,内核提供的这种机制称为进程间通信。假如创建了多个进程，那么进程间的通信是必不可少的。Python提供了多种进程通信的方式，其中以Queue和Pipe用得最多。下面分别介绍这两种模式。 QueueQueue是一种多进程安全的队列。实现多进程间的通信有两种方法： get() 用于向队列中加入数据。有两个属性：blocked和timeout。blocked为true时（默认为True）且timeout为正值时，如果当队列已满会阻塞timeout时间，在这个时间内如果队列有空位会加入，如果超过时间仍然没有空位会抛出Queue.Full异常。 put() 用于从队列中获取一个数据并将其从队列中删除。有两个属性：blocked和timeout。blocked为true（默认为True）且timeout为正值时，如果当前队列为空会阻塞timeout时间，在这个时间内如果队列有新数据会获取，如果超过时间仍然没有新数据会抛出Queue.Empty异常。1234567891011121314151617181920212223242526from multiprocessing import Process,Queueimport osdef put_data(q,nums): print('现在的进程编号为：%s，这是一个put进程' % os.getpid()) for num in nums: q.put(num) print('%d已经放入队列中啦！' % num)def get_data(q): print('现在的进程编号为：%s，这是一个get进程' % os.getpid()) while True: print('已经从队列中获取%s并从中删除' % q.get())if __name__ == '__main__': q = Queue() p1 = Process(target=put_data,args=(q,['1','2','3'],)) p2 = Process(target=put_data,args=(q,['4','5','6'],)) p3 = Process(target=get_data,args=(q,)) p1.start() p2.start() p3.start() p1.join() p2.join() # p3是个死循环，需要手动结束这个进程 p3.terminate() 我们来看一下运行结果：123456789101112131415现在的进程编号为：10336，这是一个put进程1已经放入队列中啦！2已经放入队列中啦！3已经放入队列中啦！现在的进程编号为：9116，这是一个get进程已经从队列中获取1,并从中删除已经从队列中获取2并从中删除已经从队列中获取3并从中删除现在的进程编号为：2732，这是一个put进程4已经放入队列中啦！5已经放入队列中啦！已经从队列中获取4,并从中删除6已经放入队列中啦！已经从队列中获取5并从中删除已经从队列中获取6并从中删除 PipePipe与Queue不同之处在于Pipe是用于两个进程之间的通信。就像进程位于一根水管的两端。让我们看看Pipe官方文档的描述： Returns a pair (conn1, conn2) of Connection objects representing the ends of a pipe. Piep返回conn1和conn2代表水管的两端。Pipe还有一个参数duplex（adj. 二倍的，双重的 n. 双工；占两层楼的公寓套房），默认为True。当duplex为True时，开启双工模式，此时水管的两边都可以进行收发。当duplex为False，那么conn1只负责接受信息，conn2只负责发送信息。conn通过send()和recv()来发送和接受信息。值得注意的是，如果管道中没有信息可接受，recv()会一直阻塞直到管道关闭（任意一端进程接结束则管道关闭）。123456789101112131415161718192021222324from multiprocessing import Process,Pipeimport osdef put_data(p,nums): print('现在的进程编号为：%s，这个一个send进程' % os.getpid()) for num in nums: p.send(num) print('%s已经放入管道中啦！' % num)def get_data(p): print('现在的进程编号为：%s，这个一个recv进程' % os.getpid()) while True: print('已经从管道中获取%s并从中删除' % p.recv())if __name__ == '__main__': p = Pipe(duplex=False) # 此时Pipe[1]即是Pipe返回的conn2 p1 = Process(target=put_data,args=(p[1],['1','2','3'],)) # 此时Pipe[0]即是Pipe返回的conn1 p3 = Process(target=get_data,args=(p[0],)) p1.start() p3.start() p1.join() p3.terminate() 让我们看一下输出结果12345678现在的进程编号为：9868，这个一个recv进程现在的进程编号为：9072，这个一个send进程1已经放入管道中啦！已经从管道中获取1,并从中删除2已经放入管道中啦！已经从管道中获取2并从中删除3已经放入管道中啦！已经从管道中获取3并从中删除 控制线程我们是没有办法完全人为控制线程的，因为线程由系统控制。但是可以用一些方式来影响线程的调用，比如互斥锁，sleep（阻塞），死锁等。 线程的几种状态新建—–就绪——————运行—–死亡&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;等待（阻塞）线程的生命周期由run方法决定，当run方法结束时线程死亡。可以通过继承Thread，重写run方法改变Thread的功能，最后还是通过start()方法开线程。123456789101112131415161718192021from threading import Threadclass MyThread(Thread): def run(self): print('i am sorry')if __name__ == '__main__': t = MyThread() t.start()``` 通过args参数以一个元组的方式给线程中的函数传参。 ```pythonfrom threading import Threaddef sorry(name): print('i am sorry',name)if __name__ == '__main__': t = Thread(target=sorry,args=('mike')) t.start() 线程锁多线程中任务中，可能会发生多个线程同时对一个公共资源（如全局变量）进行操作的情况，这是就会发生混乱。为了避免这种情况，需要引入线程锁的概念。只有一个线程能处于上锁状态，当一个线程上锁之后，如果有另外一个线程试图获得锁，该线程就会挂起直到拥有锁的线程将锁释放。这样就保证了同时只有一个线程对公共资源进行访问或修改。12345678910111213141516171819from threading import Thread,Locknum = 0def puls(): # 获得一个锁 lock = Lock() global num # acquire()方法上锁 lock.acquire() num += 1 print(num) # release()方法解锁 lock.release()if __name__ == '__main__': for i in range(5): t = Thread(target=plus) t.start() t.join() join()方法会阻塞主线程直到子线程全部结束（也就是同步）。锁的用处: 确保某段关键代码只能由一个线程从头到尾执行，保证了数据的唯一性。 锁的坏处: 阻止了多线程并发执行，效率大大降低。 由于存在多个锁，不同的线程持有不同的锁并试图获取对方的锁时，可能造成死锁。 守护线程线程其实并没有主次的概念，我们一般说的‘主线程’实际上是main函数的线程，而所谓主线程结束子线程也会结束是因为在主线程结束时调用了系统的退出函数。而守护线程是指‘不重要线程’。主线程会等所有‘重要’线程结束后才结束。通常当客户端访问服务器时会为这次访问开启一个守护线程。将setDaemon属性设为True即可将该线程设为守护线程。123456789101112from threading import Threadn = 100def count(x,y): return n=x+yif __name__ == '__main__': t = Thread(target=count,args=(1,2)) t.setDaemon = True # ...]]></content>
      <categories>
        <category>Web开发</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[http学习]]></title>
    <url>%2F2017%2F12%2F17%2Fhttp%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[TCP/IP与DNS什么是TCP/IP?协议中存在各式各样的内容。从电缆的规格到 IP 地址的选定方法、寻找异地用户的方法、双方建立通信的顺序，以及 Web 页面显示需要处理的步骤，等等。像这样把与互联网相关联的协议集合起来总称为 TCP/IP。HTTP 属于它内部的一个子集。也有说法认为，TCP/IP 是指 TCP 和 IP 这两种协议。还有一种说法认为，TCP/IP 是在 IP 协议的通信过程中，使用到的协议族的统称。 IPIP与IP地址不同。IP是一种网络协议，它的作用是把数据包传给对方。具体是通过MAC地址和ip地址来实现的。MAC地址对应网卡所属的固定地址，ip地址指节点被分配到的地址。IP地址可以与MAC地址对应，IP地址可以换，MAC地址一般是固定的。IP 间的通信依赖 MAC 地址。在网络上，通信的双方在同一局域网（LAN）内的情况是很少的，通常是经过多台计算机和网络设备中转才能连接到对方。而在进行中转时，会利用下一站中转设备的 MAC 地址来搜索下一个中转目标。这时，会采用 ARP 协议（AddressResolution Protocol）。ARP 是一种用以解析地址的协议，根据通信方的 IP 地址就可以反查出对应的 MAC 地址。 TCP所谓的字节流服务（Byte Stream Service）是指，为了方便传输，将大块数据分割成以报文段（segment）为单位的数据包进行管理。而可靠的传输服务是指，能够把数据准确可靠地传给对方。一言以蔽之，TCP 协议为了更容易传送大数据才把数据分割，而且 TCP 协议能够确认数据最终是否送达到对方。 TCP的三次握手TCP把数据包发出去之后还会确认数据到达了目的地，通过三次握手机制进行确认。发送端发送数据时，TCP会向服务器发送带有SYN标记的数据包，当服务器接收到这个数据包后，会返回一个带有SYN或者ASK的数据包表示确认，最后发送端会再次发送带有ASK标志的数据包表示握手结束。 DNSDNS（Domain Name System）服务是和 HTTP 协议一样位于应用层的协议。它提供域名到 IP 地址之间的解析服务。计算机既可以被赋予 IP 地址，也可以被赋予主机名和域名。比如 www.baidu.com。用户通常使用主机名或域名来访问对方的计算机，而不是直接通过 IP 地址访问。因为与 IP 地址的一组纯数字相比，用字母配合数字的表示形式来指定计算机名更符合人类的记忆习惯。但要让计算机去理解名称，相对而言就变得困难了。因为计算机更擅长处理一长串数字。为了解决上述的问题，DNS 服务应运而生。DNS 协议提供通过域名查找 IP 地址，或逆向从 IP 地址反查域名的服务。 综上，从输入网址到服务器获得请求的过程是：]]></content>
      <categories>
        <category>http</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重新开始]]></title>
    <url>%2F2017%2F12%2F02%2F%E9%87%8D%E6%96%B0%E5%BC%80%E5%A7%8B%2F</url>
    <content type="text"><![CDATA[前阵子手贱删了博客文件，还糊里糊涂地把coding里的项目也删了。然后各种蜜汁错误，各种无法重新部署。最近几天又蜜汁部署成功。可以说十分难受了。 不过塞翁失马，焉知非福。经过这么一遭我再次练习了一遍coding+hexo下博客的部署，也算是好事一桩了吧？（强行自我安慰一波233）以后我一定天天向上，重新做人，再不手贱。 再次感谢王哥的教程，很详细，帮助很大，很好，很棒。感兴趣的同志可以去他那里转转呀（手动滑稽）windliang的博客扯到这里算是暂时结束，以后有时间再上来扯扯淡，写点学习心得啥的吧。]]></content>
      <categories>
        <category>心得</category>
      </categories>
      <tags>
        <tag>闲聊</tag>
      </tags>
  </entry>
</search>
